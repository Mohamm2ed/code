{"cells":[{"cell_type":"markdown","metadata":{"id":"GSB3vgp6SaH2"},"source":["### Installation"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"hIojkLKGSaH3","executionInfo":{"status":"ok","timestamp":1748525581551,"user_tz":-60,"elapsed":12201,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}}},"outputs":[],"source":["%%capture\n","import os\n","if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","    !pip install unsloth\n","else:\n","    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n","    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n","    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n","    !pip install transformers==4.51.3\n","    !pip install --no-deps unsloth\n","!git clone https://github.com/SparkAudio/Spark-TTS\n","!pip install omegaconf einx"]},{"cell_type":"markdown","metadata":{"id":"AkWYsztAs9Ky"},"source":["### Unsloth\n","\n","`FastModel` supports loading nearly any model now! This includes Vision and Text models!\n","\n","Thank you to [Etherl](https://huggingface.co/Etherll) for creating this notebook!"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2025-03-22T00:48:54.511089Z","iopub.status.busy":"2025-03-22T00:48:54.510770Z","iopub.status.idle":"2025-03-22T00:51:37.363415Z","shell.execute_reply":"2025-03-22T00:51:37.362696Z","shell.execute_reply.started":"2025-03-22T00:48:54.511053Z"},"id":"QmUBVEnvCDJv","outputId":"4f846cdd-2c46-49fc-9553-5d7de6ee1b98","trusted":true,"executionInfo":{"status":"ok","timestamp":1748525607544,"user_tz":-60,"elapsed":23511,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.5.8: Fast Qwen2 patching. Transformers: 4.51.3.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","Unsloth: Float16 full finetuning uses more memory since we upcast weights to float32.\n"]}],"source":["from unsloth import FastModel\n","import torch\n","from huggingface_hub import snapshot_download\n","\n","max_seq_length = 2048 # Choose any for long context!\n","\n","fourbit_models = [\n","    # 4bit dynamic quants for superior accuracy and low memory use\n","    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n","    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n","    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n","    # Qwen3 new models\n","    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n","    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n","    # Other very popular models!\n","    \"unsloth/Llama-3.1-8B\",\n","    \"unsloth/Llama-3.2-3B\",\n","    \"unsloth/Llama-3.3-70B\",\n","    \"unsloth/mistral-7b-instruct-v0.3\",\n","    \"unsloth/Phi-4\",\n","] # More models at https://huggingface.co/unsloth\n","\n","# Download model and code\n","snapshot_download(\"unsloth/Spark-TTS-0.5B\", local_dir = \"Spark-TTS-0.5B\")\n","\n","model, tokenizer = FastModel.from_pretrained(\n","    model_name = f\"Spark-TTS-0.5B/LLM\",\n","    max_seq_length = max_seq_length,\n","    dtype = torch.float32, # Spark seems to only work on float32 for now\n","    full_finetuning = True, # We support full finetuning now!\n","    load_in_4bit = False,\n","    #token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",")"]},{"cell_type":"markdown","metadata":{"id":"SXd9bTZd1aaL"},"source":["We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2025-03-22T00:51:37.365079Z","iopub.status.busy":"2025-03-22T00:51:37.364731Z","iopub.status.idle":"2025-03-22T00:51:44.221612Z","shell.execute_reply":"2025-03-22T00:51:44.220949Z","shell.execute_reply.started":"2025-03-22T00:51:37.365045Z"},"id":"6bZsfBuZDeCL","outputId":"5a73bee4-65fb-4344-ad62-d48c980d237a","trusted":true,"executionInfo":{"status":"ok","timestamp":1748525609995,"user_tz":-60,"elapsed":44,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Unsloth: Full finetuning is enabled, so .get_peft_model has no effect\n"]}],"source":["#LoRA does not work with float32 only works with bfloat16 !!!\n","model = FastModel.get_peft_model(\n","    model,\n","    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 128,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,  # We support rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n",")"]},{"cell_type":"markdown","metadata":{"id":"vITh0KVJ10qX"},"source":["<a name=\"Data\"></a>\n","### Data Prep  \n","\n","We will use the `MrDragonFox/Elise`, which is designed for training TTS models. Ensure that your dataset follows the required format: **text, audio** for single-speaker models or **source, text, audio** for multi-speaker models. You can modify this section to accommodate your own dataset, but maintaining the correct structure is essential for optimal training."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2025-03-22T00:51:44.222880Z","iopub.status.busy":"2025-03-22T00:51:44.222617Z","iopub.status.idle":"2025-03-22T00:52:16.516878Z","shell.execute_reply":"2025-03-22T00:52:16.516033Z","shell.execute_reply.started":"2025-03-22T00:51:44.222848Z"},"id":"LjY75GoYUCB8","trusted":true,"executionInfo":{"status":"ok","timestamp":1748525612929,"user_tz":-60,"elapsed":967,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}}},"outputs":[],"source":["from datasets import load_dataset\n","dataset = load_dataset(\"MrDragonFox/Elise\", split = \"train\")"]},{"cell_type":"code","execution_count":18,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["c3a0b78b654741f5bda7fed73fed11fb","e76904796c34499daa6829d2c5dd53bf","2c192ef4235a4a158ba9f7d05b9fb6d1","1431ed7c778e45269a69cd69a0ed0541","c0b1c2e51e5546cb9715e1158ac63c94","bde73257b2174f999fe81e4760daa161","e19a698ddf5d417191dab328cd0123e7","9df4928258c64ab1b889e863eb5ca9bd","c40b5db7e86a42359fa48cdb0795f355","141c4fdb0ce942ad99d9f030e6d9601a","0416d74accc2480181e599dc9d02a63d"]},"execution":{"iopub.execute_input":"2025-03-22T00:52:16.518175Z","iopub.status.busy":"2025-03-22T00:52:16.517841Z","iopub.status.idle":"2025-03-22T00:52:35.039329Z","shell.execute_reply":"2025-03-22T00:52:35.038356Z","shell.execute_reply.started":"2025-03-22T00:52:16.518146Z"},"id":"zK94B-Pfioto","outputId":"bf3c6042-4453-497a-c439-c2123a078d57","trusted":true,"executionInfo":{"status":"ok","timestamp":1748525795423,"user_tz":-60,"elapsed":179981,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n","  WeightNorm.apply(module, name, dim)\n"]},{"output_type":"stream","name":"stdout","text":["Missing tensor: mel_transformer.spectrogram.window\n","Missing tensor: mel_transformer.mel_scale.fb\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1195 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3a0b78b654741f5bda7fed73fed11fb"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Moving Bicodec model and Wav2Vec2Model to cpu.\n"]}],"source":["#@title Tokenization Function\n","\n","import locale\n","import torchaudio.transforms as T\n","import os\n","import torch\n","import sys\n","import numpy as np\n","sys.path.append('Spark-TTS')\n","from sparktts.models.audio_tokenizer import BiCodecTokenizer\n","from sparktts.utils.audio import audio_volume_normalize\n","\n","audio_tokenizer = BiCodecTokenizer(\"Spark-TTS-0.5B\", \"cuda\")\n","def extract_wav2vec2_features( wavs: torch.Tensor) -> torch.Tensor:\n","        \"\"\"extract wav2vec2 features\"\"\"\n","\n","        if wavs.shape[0] != 1:\n","\n","             raise ValueError(f\"Expected batch size 1, but got shape {wavs.shape}\")\n","        wav_np = wavs.squeeze(0).cpu().numpy()\n","\n","        processed = audio_tokenizer.processor(\n","            wav_np,\n","            sampling_rate=16000,\n","            return_tensors=\"pt\",\n","            padding=True,\n","        )\n","        input_values = processed.input_values\n","\n","        input_values = input_values.to(audio_tokenizer.feature_extractor.device)\n","\n","        model_output = audio_tokenizer.feature_extractor(\n","            input_values,\n","        )\n","\n","\n","        if model_output.hidden_states is None:\n","             raise ValueError(\"Wav2Vec2Model did not return hidden states. Ensure config `output_hidden_states=True`.\")\n","\n","        num_layers = len(model_output.hidden_states)\n","        required_layers = [11, 14, 16]\n","        if any(l >= num_layers for l in required_layers):\n","             raise IndexError(f\"Requested hidden state indices {required_layers} out of range for model with {num_layers} layers.\")\n","\n","        feats_mix = (\n","            model_output.hidden_states[11] + model_output.hidden_states[14] + model_output.hidden_states[16]\n","        ) / 3\n","\n","        return feats_mix\n","def formatting_audio_func(example):\n","    text = f\"{example['source']}: {example['text']}\" if \"source\" in example else example[\"text\"]\n","    audio_array = example[\"audio\"][\"array\"]\n","    sampling_rate = example[\"audio\"][\"sampling_rate\"]\n","\n","    target_sr = audio_tokenizer.config['sample_rate']\n","\n","    if sampling_rate != target_sr:\n","        resampler = T.Resample(orig_freq=sampling_rate, new_freq=target_sr)\n","        audio_tensor_temp = torch.from_numpy(audio_array).float()\n","        audio_array = resampler(audio_tensor_temp).numpy()\n","\n","    if audio_tokenizer.config[\"volume_normalize\"]:\n","        audio_array = audio_volume_normalize(audio_array)\n","\n","    ref_wav_np = audio_tokenizer.get_ref_clip(audio_array)\n","\n","    audio_tensor = torch.from_numpy(audio_array).unsqueeze(0).float().to(audio_tokenizer.device)\n","    ref_wav_tensor = torch.from_numpy(ref_wav_np).unsqueeze(0).float().to(audio_tokenizer.device)\n","\n","\n","    feat = extract_wav2vec2_features(audio_tensor)\n","\n","    batch = {\n","\n","        \"wav\": audio_tensor,\n","        \"ref_wav\": ref_wav_tensor,\n","        \"feat\": feat.to(audio_tokenizer.device),\n","    }\n","\n","\n","    semantic_token_ids, global_token_ids = audio_tokenizer.model.tokenize(batch)\n","\n","    global_tokens = \"\".join(\n","        [f\"<|bicodec_global_{i}|>\" for i in global_token_ids.squeeze().cpu().numpy()] # Squeeze batch dim\n","    )\n","    semantic_tokens = \"\".join(\n","        [f\"<|bicodec_semantic_{i}|>\" for i in semantic_token_ids.squeeze().cpu().numpy()] # Squeeze batch dim\n","    )\n","\n","    inputs = [\n","        \"<|task_tts|>\",\n","        \"<|start_content|>\",\n","        text,\n","        \"<|end_content|>\",\n","        \"<|start_global_token|>\",\n","        global_tokens,\n","        \"<|end_global_token|>\",\n","        \"<|start_semantic_token|>\",\n","        semantic_tokens,\n","        \"<|end_semantic_token|>\",\n","        \"<|im_end|>\"\n","    ]\n","    inputs = \"\".join(inputs)\n","    return {\"text\": inputs}\n","\n","\n","dataset = dataset.map(formatting_audio_func, remove_columns=[\"audio\"])\n","print(\"Moving Bicodec model and Wav2Vec2Model to cpu.\")\n","audio_tokenizer.model.cpu()\n","audio_tokenizer.feature_extractor.cpu()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"idAEIeSQ3xdS"},"source":["<a name=\"Train\"></a>\n","### Train the model\n","Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"execution":{"iopub.execute_input":"2025-03-22T00:34:09.688959Z","iopub.status.busy":"2025-03-22T00:34:09.688649Z","iopub.status.idle":"2025-03-22T00:34:09.729661Z","shell.execute_reply":"2025-03-22T00:34:09.729001Z","shell.execute_reply.started":"2025-03-22T00:34:09.688939Z"},"id":"95_Nn-89DhsL","outputId":"f5bbd354-6c83-459d-dc74-8b2809da3a80","trusted":true,"executionInfo":{"status":"error","timestamp":1748525899093,"user_tz":-60,"elapsed":193,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}}},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Your setup doesn't support bf16/gpu. You need torch>=1.10, using Ampere GPU with cuda>=11.0","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-c2a839e2111e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_bfloat16_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m trainer = SFTTrainer(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/trainer.py\u001b[0m in \u001b[0;36mnew_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0moriginal_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func, **kwargs)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[0mfix_zero_training_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m   1005\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mdict_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hub_token\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_token\u001b[0m  \u001b[0;31m# to_dict hides the hub_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mdict_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"push_to_hub_token\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSFTConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;31m# Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_config.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, tp_size, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_arg...\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_config.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__post_init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__post_init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1705\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_bf16_gpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m                         \u001b[0;31m# gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1707\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m   1708\u001b[0m                             \u001b[0;34m\"Your setup doesn't support bf16/gpu. You need torch>=1.10, using Ampere GPU with cuda>=11.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m                         )\n","\u001b[0;31mValueError\u001b[0m: Your setup doesn't support bf16/gpu. You need torch>=1.10, using Ampere GPU with cuda>=11.0"]}],"source":["from trl import SFTTrainer\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","    packing = False, # Can make training 5x faster for short sequences.\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4,\n","        warmup_steps = 5,\n","        # num_train_epochs = 1, # Set this for 1 full training run.\n","        max_steps = 60,\n","        learning_rate = 2e-4,\n","        fp16 = False, # We're doing full float32 s disable mixed precision\n","        bf16 = False, # We're doing full float32 s disable mixed precision\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","        report_to = \"none\", # Use this for WandB etc\n","    ),\n",")"]},{"cell_type":"code","source":["import torch\n","print(torch.__version__)\n","print(torch.cuda.is_available())\n","print(torch.cuda.get_device_name(0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UqTBLcoaQr3G","executionInfo":{"status":"ok","timestamp":1748525827507,"user_tz":-60,"elapsed":62,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}},"outputId":"444a4615-5823-43f0-e3b9-77d9a3a8b1ef"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["2.6.0+cu124\n","True\n","Tesla T4\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"2ejIt2xSNKKp","executionInfo":{"status":"aborted","timestamp":1748525074566,"user_tz":-60,"elapsed":9,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}}},"outputs":[],"source":["# @title Show current memory stats\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"execution":{"iopub.execute_input":"2025-03-22T00:34:12.049152Z","iopub.status.busy":"2025-03-22T00:34:12.048862Z","iopub.status.idle":"2025-03-22T00:34:14.404349Z","shell.execute_reply":"2025-03-22T00:34:14.403239Z","shell.execute_reply.started":"2025-03-22T00:34:12.049130Z"},"id":"yqxqAZ7KJ4oL","outputId":"6f8cf11a-6df2-42d1-d285-3c57d4805ab8","trusted":true,"executionInfo":{"status":"error","timestamp":1748525232574,"user_tz":-60,"elapsed":56,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'trainer' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-3d62c575fcfd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"]}],"source":["trainer_stats = trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"pCqnaKmlO1U9","executionInfo":{"status":"aborted","timestamp":1748525074615,"user_tz":-60,"elapsed":365197,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}}},"outputs":[],"source":["# @title Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory / max_memory * 100, 3)\n","lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(\n","    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"]},{"cell_type":"markdown","metadata":{"id":"ekOmTR1hSNcr"},"source":["<a name=\"Inference\"></a>\n","### Inference\n","Let's run the model! You can change the prompts\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"apUdB40Ep6Ki","executionInfo":{"status":"aborted","timestamp":1748525074629,"user_tz":-60,"elapsed":362729,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}}},"outputs":[],"source":["input_text = \"Hey there my name is Elise, <giggles> and I'm a speech generation model that can sound like a person.\"\n","\n","chosen_voice = None # None for single-speaker"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-22T00:52:35.040842Z","iopub.status.busy":"2025-03-22T00:52:35.040125Z","iopub.status.idle":"2025-03-22T00:52:35.050560Z","shell.execute_reply":"2025-03-22T00:52:35.049663Z","shell.execute_reply.started":"2025-03-22T00:52:35.040818Z"},"id":"krYI8PrRJ6MX","trusted":true,"executionInfo":{"status":"aborted","timestamp":1748525074637,"user_tz":-60,"elapsed":361875,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}}},"outputs":[],"source":["#@title Run Inference\n","\n","import torch\n","import re\n","import numpy as np\n","from typing import Dict, Any\n","import torchaudio.transforms as T\n","\n","FastModel.for_inference(model) # Enable native 2x faster inference\n","\n","@torch.inference_mode()\n","def generate_speech_from_text(\n","    text: str,\n","    temperature: float = 0.8,   # Generation temperature\n","    top_k: int = 50,            # Generation top_k\n","    top_p: float = 1,        # Generation top_p\n","    max_new_audio_tokens: int = 2048, # Max tokens for audio part\n","    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",") -> np.ndarray:\n","    \"\"\"\n","    Generates speech audio from text using default voice control parameters.\n","\n","    Args:\n","        text (str): The text input to be converted to speech.\n","        temperature (float): Sampling temperature for generation.\n","        top_k (int): Top-k sampling parameter.\n","        top_p (float): Top-p (nucleus) sampling parameter.\n","        max_new_audio_tokens (int): Max number of new tokens to generate (limits audio length).\n","        device (torch.device): Device to run inference on.\n","\n","    Returns:\n","        np.ndarray: Generated waveform as a NumPy array.\n","    \"\"\"\n","\n","    prompt = \"\".join([\n","        \"<|task_tts|>\",\n","        \"<|start_content|>\",\n","        text,\n","        \"<|end_content|>\",\n","        \"<|start_global_token|>\"\n","    ])\n","\n","    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n","\n","    print(\"Generating token sequence...\")\n","    generated_ids = model.generate(\n","        **model_inputs,\n","        max_new_tokens=max_new_audio_tokens, # Limit generation length\n","        do_sample=True,\n","        temperature=temperature,\n","        top_k=top_k,\n","        top_p=top_p,\n","        eos_token_id=tokenizer.eos_token_id, # Stop token\n","        pad_token_id=tokenizer.pad_token_id # Use models pad token id\n","    )\n","    print(\"Token sequence generated.\")\n","\n","\n","    generated_ids_trimmed = generated_ids[:, model_inputs.input_ids.shape[1]:]\n","\n","\n","    predicts_text = tokenizer.batch_decode(generated_ids_trimmed, skip_special_tokens=False)[0]\n","    # print(f\"\\nGenerated Text (for parsing):\\n{predicts_text}\\n\") # Debugging\n","\n","    # Extract semantic token IDs using regex\n","    semantic_matches = re.findall(r\"<\\|bicodec_semantic_(\\d+)\\|>\", predicts_text)\n","    if not semantic_matches:\n","        print(\"Warning: No semantic tokens found in the generated output.\")\n","        # Handle appropriately - perhaps return silence or raise error\n","        return np.array([], dtype=np.float32)\n","\n","    pred_semantic_ids = torch.tensor([int(token) for token in semantic_matches]).long().unsqueeze(0) # Add batch dim\n","\n","    # Extract global token IDs using regex (assuming controllable mode also generates these)\n","    global_matches = re.findall(r\"<\\|bicodec_global_(\\d+)\\|>\", predicts_text)\n","    if not global_matches:\n","         print(\"Warning: No global tokens found in the generated output (controllable mode). Might use defaults or fail.\")\n","         pred_global_ids = torch.zeros((1, 1), dtype=torch.long)\n","    else:\n","         pred_global_ids = torch.tensor([int(token) for token in global_matches]).long().unsqueeze(0) # Add batch dim\n","\n","    pred_global_ids = pred_global_ids.unsqueeze(0) # Shape becomes (1, 1, N_global)\n","\n","    print(f\"Found {pred_semantic_ids.shape[1]} semantic tokens.\")\n","    print(f\"Found {pred_global_ids.shape[2]} global tokens.\")\n","\n","\n","    # 5. Detokenize using BiCodecTokenizer\n","    print(\"Detokenizing audio tokens...\")\n","    # Ensure audio_tokenizer and its internal model are on the correct device\n","    audio_tokenizer.device = device\n","    audio_tokenizer.model.to(device)\n","    # Squeeze the extra dimension from global tokens as seen in SparkTTS example\n","    wav_np = audio_tokenizer.detokenize(\n","        pred_global_ids.to(device).squeeze(0), # Shape (1, N_global)\n","        pred_semantic_ids.to(device)           # Shape (1, N_semantic)\n","    )\n","    print(\"Detokenization complete.\")\n","\n","    return wav_np\n","\n","if __name__ == \"__main__\":\n","    print(f\"Generating speech for: '{input_text}'\")\n","    text = f\"{chosen_voice}: \" + input_text if chosen_voice else input_text\n","    generated_waveform = generate_speech_from_text(input_text)\n","\n","    if generated_waveform.size > 0:\n","        import soundfile as sf\n","        output_filename = \"generated_speech_controllable.wav\"\n","        sample_rate = audio_tokenizer.config.get(\"sample_rate\", 16000)\n","        sf.write(output_filename, generated_waveform, sample_rate)\n","        print(f\"Audio saved to {output_filename}\")\n","\n","        # Optional: Play in notebook\n","        from IPython.display import Audio, display\n","        display(Audio(generated_waveform, rate=sample_rate))\n","    else:\n","        print(\"Audio generation failed (no tokens found?).\")"]},{"cell_type":"markdown","metadata":{"id":"uMuVrWbjAzhc"},"source":["<a name=\"Save\"></a>\n","### Saving, loading finetuned models\n","To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n","\n","**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"upcOlWe7A1vc","executionInfo":{"status":"aborted","timestamp":1748525074647,"user_tz":-60,"elapsed":358386,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}}},"outputs":[],"source":["model.save_pretrained(\"semantic_lora_modell\")  # Local saving\n","tokenizer.save_pretrained(\"semantic_lora_modell\")\n","model.push_to_hub(\"m3nnoun/semantic_lora_model\", token = \"...\") # Online saving\n","tokenizer.push_to_hub(\"m3nnoun/semantic_lora_modell\", token = \"...\") # Online saving"]},{"cell_type":"markdown","metadata":{"id":"f422JgM9sdVT"},"source":["### Saving to float16\n","\n","We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHjt_SMYsd3P","executionInfo":{"status":"aborted","timestamp":1748525074667,"user_tz":-60,"elapsed":355567,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}}},"outputs":[],"source":["# Merge to 16bit\n","if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n","if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n","\n","# Merge to 4bit\n","if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n","if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n","\n","# Just LoRA adapters\n","if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n","if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"]},{"cell_type":"code","source":["## sending the data using the api\n","!pip install flask pyngrok gTTS flask_cors"],"metadata":{"id":"8_MeKYnPTMFH","executionInfo":{"status":"aborted","timestamp":1748525074681,"user_tz":-60,"elapsed":353743,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from flask import Flask, request, send_file, jsonify\n","from flask_cors import CORS\n","from pyngrok import ngrok, conf\n","import tempfile, os, threading\n","import soundfile as sf\n","\n","# Your existing imports for speech generation\n","# from your_speech_model import generate_speech_from_text, audio_tokenizer, chosen_voice\n","\n","from google.colab import userdata\n","NGROK_TOKEN = userdata.get('ngrok')\n","conf.get_default().auth_token = NGROK_TOKEN\n","\n","app = Flask(__name__)\n","\n","# More specific CORS configuration\n","CORS(app, resources={\n","    r\"/text-to-speech\": {\n","        \"origins\": \"*\",\n","        \"methods\": [\"POST\", \"OPTIONS\"],\n","        \"allow_headers\": [\"Content-Type\", \"ngrok-skip-browser-warning\"]\n","    }\n","})\n","\n","@app.route('/text-to-speech', methods=['POST', 'OPTIONS'])\n","def text_to_speech():\n","    # Handle preflight OPTIONS request\n","    if request.method == 'OPTIONS':\n","        response = jsonify({'status': 'ok'})\n","        response.headers.add('Access-Control-Allow-Origin', '*')\n","        response.headers.add('Access-Control-Allow-Headers', 'Content-Type, ngrok-skip-browser-warning')\n","        response.headers.add('Access-Control-Allow-Methods', 'POST, OPTIONS')\n","        return response\n","\n","    try:\n","        data = request.get_json()\n","        if not data or 'text' not in data:\n","            return jsonify({\"error\": \"No text provided\"}), 400\n","\n","        if not data['text'].strip():\n","            return jsonify({\"error\": \"Text cannot be empty\"}), 400\n","\n","        # Get the input text from the request\n","        input_text = data['text']\n","\n","        # Your main function logic integrated here\n","        print(f\"Generating speech for: '{input_text}'\")\n","\n","        # Prepare text with voice if needed\n","        text = f\"{chosen_voice}: \" + input_text if chosen_voice else input_text\n","\n","        # Generate speech using your custom model instead of gTTS\n","        generated_waveform = generate_speech_from_text(input_text)\n","\n","        if generated_waveform.size > 0:\n","            # Create temporary file for the generated audio\n","            with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as fp:\n","                output_filename = fp.name\n","\n","                # Get sample rate from your audio tokenizer\n","                sample_rate = audio_tokenizer.config.get(\"sample_rate\", 16000)\n","\n","                # Save the generated waveform\n","                sf.write(output_filename, generated_waveform, sample_rate)\n","                print(f\"Audio saved to {output_filename}\")\n","\n","            # Send file and then clean up\n","            response = send_file(\n","                output_filename,\n","                mimetype='audio/wav',\n","                as_attachment=False,\n","                download_name=\"generated_speech.wav\"\n","            )\n","\n","            # Clean up file after sending\n","            @response.call_on_close\n","            def cleanup():\n","                try:\n","                    os.unlink(output_filename)\n","                except Exception:\n","                    pass\n","\n","            return response\n","        else:\n","            print(\"Audio generation failed (no tokens found?).\")\n","            return jsonify({\"error\": \"Audio generation failed (no tokens found?)\"}), 500\n","\n","    except Exception as e:\n","        print(f\"Error in speech generation: {str(e)}\")\n","        return jsonify({\"error\": str(e)}), 500\n","\n","@app.route('/health', methods=['GET'])\n","def health_check():\n","    return jsonify({\"status\": \"ok\", \"message\": \"Custom TTS service is running\"})\n","\n","def run_app():\n","    app.run(host='0.0.0.0', port=5000, debug=False)\n","\n","# Start Flask app in a thread\n","threading.Thread(target=run_app, daemon=True).start()\n","\n","# Start ngrok\n","public_url = ngrok.connect(5000)\n","print(f\"✅ Public URL: {public_url}\")\n","print(f\"✅ Health check: {public_url}/health\")\n","print(\"Server is running...\")"],"metadata":{"id":"H4j0MzwrMSId","executionInfo":{"status":"aborted","timestamp":1748525074726,"user_tz":-60,"elapsed":352235,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","for i in range(100):\n","  time.sleep(30)\n","  print(i)"],"metadata":{"id":"jU6By3YeMm4p","executionInfo":{"status":"aborted","timestamp":1748525074749,"user_tz":-60,"elapsed":315212,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from flask import Flask, request, send_file, jsonify\n","from gtts import gTTS\n","from pyngrok import ngrok, conf\n","import tempfile, os, threading\n","from flask_cors import CORS\n","\n","from google.colab import userdata\n","NGROK_TOKEN = userdata.get('ngrok')\n","conf.get_default().auth_token = NGROK_TOKEN\n","\n","app = Flask(__name__)\n","\n","# More specific CORS configuration\n","CORS(app, resources={\n","    r\"/text-to-speech\": {\n","        \"origins\": \"*\",\n","        \"methods\": [\"POST\", \"OPTIONS\"],\n","        \"allow_headers\": [\"Content-Type\", \"ngrok-skip-browser-warning\"]\n","    }\n","})\n","\n","@app.route('/text-to-speech', methods=['POST', 'OPTIONS'])\n","def text_to_speech():\n","    # Handle preflight OPTIONS request\n","    if request.method == 'OPTIONS':\n","        response = jsonify({'status': 'ok'})\n","        response.headers.add('Access-Control-Allow-Origin', '*')\n","        response.headers.add('Access-Control-Allow-Headers', 'Content-Type, ngrok-skip-browser-warning')\n","        response.headers.add('Access-Control-Allow-Methods', 'POST, OPTIONS')\n","        return response\n","\n","    try:\n","        data = request.get_json()\n","        if not data or 'text' not in data:\n","            return jsonify({\"error\": \"No text provided\"}), 400\n","\n","        if not data['text'].strip():\n","            return jsonify({\"error\": \"Text cannot be empty\"}), 400\n","\n","        # Generate TTS\n","        tts = gTTS(text=data['text'], lang='en')\n","\n","        # Create temporary file\n","        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as fp:\n","            tts.save(fp.name)\n","            audio_path = fp.name\n","\n","        # Send file and then clean up\n","        def remove_file(response):\n","            try:\n","                os.unlink(audio_path)\n","            except Exception:\n","                pass\n","            return response\n","\n","        response = send_file(\n","            audio_path,\n","            mimetype='audio/mpeg',\n","            as_attachment=False,\n","            download_name=\"speech.mp3\"\n","        )\n","\n","        # Clean up file after sending\n","        @response.call_on_close\n","        def cleanup():\n","            try:\n","                os.unlink(audio_path)\n","            except Exception:\n","                pass\n","\n","        return response\n","\n","    except Exception as e:\n","        return jsonify({\"error\": str(e)}), 500\n","\n","@app.route('/health', methods=['GET'])\n","def health_check():\n","    return jsonify({\"status\": \"ok\", \"message\": \"TTS service is running\"})\n","\n","def run_app():\n","    app.run(host='0.0.0.0', port=5000, debug=False)\n","\n","# Start Flask app in a thread\n","threading.Thread(target=run_app, daemon=True).start()\n","\n","# Start ngrok\n","public_url = ngrok.connect(5000)\n","print(f\"✅ Public URL: {public_url}\")\n","print(f\"✅ Health check: {public_url}/health\")\n","print(\"Server is running...\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxzbnwwATV1t","executionInfo":{"status":"ok","timestamp":1748512394494,"user_tz":-60,"elapsed":5090,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}},"outputId":"ca7515fd-261d-42b5-cc1b-7ecb43c68652"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading ngrok ...\r * Serving Flask app '__main__'\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n"," * Running on all addresses (0.0.0.0)\n"," * Running on http://127.0.0.1:5000\n"," * Running on http://172.28.0.12:5000\n","INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["✅ Public URL: NgrokTunnel: \"https://7572-34-124-205-16.ngrok-free.app\" -> \"http://localhost:5000\"\n","✅ Health check: NgrokTunnel: \"https://7572-34-124-205-16.ngrok-free.app\" -> \"http://localhost:5000\"/health\n","Server is running...\n"]}]},{"cell_type":"code","source":["import time\n","\n","for i in range(100):\n","  time.sleep(30)\n","  print(i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":852},"id":"9_MUkQ8GTWef","executionInfo":{"status":"error","timestamp":1748480845365,"user_tz":-60,"elapsed":839625,"user":{"displayName":"Abdelfatah M","userId":"05712709112360477411"}},"outputId":"e02e2c7c-ed73-44fc-d17f-459a8989efcb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [29/May/2025 00:57:09] \"OPTIONS /text-to-speech HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [29/May/2025 00:57:12] \"OPTIONS /text-to-speech HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["7\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [29/May/2025 00:57:30] \"OPTIONS /text-to-speech HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [29/May/2025 00:57:33] \"OPTIONS /text-to-speech HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [29/May/2025 01:02:01] \"OPTIONS /text-to-speech HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [29/May/2025 01:02:20] \"\u001b[31m\u001b[1mGET /text-to-speech HTTP/1.1\u001b[0m\" 405 -\n","INFO:werkzeug:127.0.0.1 - - [29/May/2025 01:02:21] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"]},{"output_type":"stream","name":"stdout","text":["17\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [29/May/2025 01:02:27] \"OPTIONS /text-to-speech HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [29/May/2025 01:02:30] \"OPTIONS /text-to-speech HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [29/May/2025 01:02:33] \"OPTIONS /text-to-speech HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [29/May/2025 01:02:51] \"OPTIONS /text-to-speech HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-809027a28c28>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"0I1uP7Zah4_m"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c3a0b78b654741f5bda7fed73fed11fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e76904796c34499daa6829d2c5dd53bf","IPY_MODEL_2c192ef4235a4a158ba9f7d05b9fb6d1","IPY_MODEL_1431ed7c778e45269a69cd69a0ed0541"],"layout":"IPY_MODEL_c0b1c2e51e5546cb9715e1158ac63c94"}},"e76904796c34499daa6829d2c5dd53bf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bde73257b2174f999fe81e4760daa161","placeholder":"​","style":"IPY_MODEL_e19a698ddf5d417191dab328cd0123e7","value":"Map: 100%"}},"2c192ef4235a4a158ba9f7d05b9fb6d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9df4928258c64ab1b889e863eb5ca9bd","max":1195,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c40b5db7e86a42359fa48cdb0795f355","value":1195}},"1431ed7c778e45269a69cd69a0ed0541":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_141c4fdb0ce942ad99d9f030e6d9601a","placeholder":"​","style":"IPY_MODEL_0416d74accc2480181e599dc9d02a63d","value":" 1195/1195 [02:39&lt;00:00,  7.84 examples/s]"}},"c0b1c2e51e5546cb9715e1158ac63c94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bde73257b2174f999fe81e4760daa161":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e19a698ddf5d417191dab328cd0123e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9df4928258c64ab1b889e863eb5ca9bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c40b5db7e86a42359fa48cdb0795f355":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"141c4fdb0ce942ad99d9f030e6d9601a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0416d74accc2480181e599dc9d02a63d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}